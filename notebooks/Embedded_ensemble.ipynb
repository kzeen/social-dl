{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e679837e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2025/prakhar.tiwari/miniconda3/envs/CV_Project/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ==== Basic Python / Data Handling ====\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ==== Scikit-Learn Utilities ====\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# ==== PyTorch ====\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# ==== HuggingFace Transformers ====\n",
    "from transformers import (\n",
    "    CamembertTokenizer,\n",
    "    CamembertForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from transformers import CamembertModel\n",
    "\n",
    "\n",
    "# ==== LightGBM (Metadata Model) ====\n",
    "import lightgbm as lgb\n",
    "\n",
    "# ==== Progress Bar (optional but recommended) ====\n",
    "from tqdm import tqdm\n",
    "\n",
    "# ==== Warnings (optional) ====\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f2c31e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_jsonl(path):\n",
    "    df = pd.read_json(path, lines=True)\n",
    "\n",
    "    # --- Clean text ---\n",
    "    def clean_text(t):\n",
    "        if pd.isna(t):\n",
    "            return \"\"\n",
    "        t = str(t).replace(\"\\n\", \" \").strip()\n",
    "        return re.sub(r\"\\s+\", \" \", t)\n",
    "\n",
    "    df[\"text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "    # --- Text features Extraction---\n",
    "    df[\"has_url\"] = df[\"text\"].str.contains(r\"http[s]?://\", regex=True).astype(int)\n",
    "    df[\"num_hashtags\"] = df[\"text\"].str.count(r\"#\")\n",
    "    df[\"has_hashtag\"] = (df[\"num_hashtags\"] > 0).astype(int)\n",
    "    df[\"num_mentions\"] = df[\"text\"].str.count(r\"@\")\n",
    "\n",
    "    emoji_pattern = r\"[\\U0001F600-\\U0001F64F]\"\n",
    "    df[\"num_emojis\"] = df[\"text\"].str.count(emoji_pattern)\n",
    "\n",
    "    df[\"text_len\"] = df[\"text\"].str.len()\n",
    "    df[\"num_caps\"] = df[\"text\"].str.count(r\"[A-Z]\")\n",
    "    df[\"num_exclam\"] = df[\"text\"].str.count(r\"!\")\n",
    "    df[\"num_question\"] = df[\"text\"].str.count(r\"\\?\")\n",
    "    df[\"elongated_words\"] = df[\"text\"].str.contains(r\"(.)\\1\\1+\").astype(int)\n",
    "\n",
    "    df[\"emoji_density\"] = df[\"num_emojis\"] / (df[\"text_len\"] + 1)\n",
    "    df[\"punct_ratio\"] = (df[\"num_exclam\"] + df[\"num_question\"]) / (df[\"text_len\"] + 1)\n",
    "\n",
    "\n",
    "    df[\"user_created_at\"] = df[\"user\"].apply(\n",
    "    lambda u: u.get(\"created_at\", None) if isinstance(u, dict) else None)\n",
    "\n",
    "    df['hour'] = pd.to_datetime(df['created_at'], errors='coerce').dt.hour.fillna(-1).astype(int)\n",
    "    df['day_of_week'] = pd.to_datetime(df['created_at'], errors='coerce').dt.dayofweek.fillna(-1).astype(int)\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    df['is_business_hours'] = df['hour'].between(9, 17).astype(int)\n",
    "    def part_of_day(h):\n",
    "        if h == -1: return -1\n",
    "        if 5 <= h < 12:  return 0   # morning\n",
    "        if 12 <= h < 17: return 1   # afternoon\n",
    "        if 17 <= h < 21: return 2   # evening (prime time)\n",
    "        return 3                    # night\n",
    "\n",
    "    df['part_of_day'] = df['hour'].apply(part_of_day)\n",
    "    df['month'] = pd.to_datetime(df['created_at'], errors='coerce').dt.month.fillna(-1).astype(int)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # --- Promo words ---\n",
    "    promo_words = [\n",
    "    # English\n",
    "    \"check out\", \"giveaway\", \"subscribe\", \"new video\", \"follow me\", \"promo\",\n",
    "    \"discount\", \"code\", \"link in bio\", \"limited offer\", \"free shipping\",\n",
    "    \"new post\", \"sale\", \"deal\", \"official store\", \"use my code\", \"partnership\",\n",
    "    \"affiliate\", \"sponsored\", \"collab\", \"win\", \"contest\", \"flash sale\",\n",
    "    \"click here\", \"buy now\", \"shop now\", \"big announcement\", \"launch\",\n",
    "    \"premiere\", \"live now\", \"join now\", \"special offer\",\n",
    "\n",
    "    # French\n",
    "    \"nouvelle vidéo\", \"abonnez\", \"abonnez-vous\", \"nouveau post\", \"concours\",\n",
    "    \"gagnez\", \"offre\", \"code promo\", \"réduction\", \"promo\", \"découvrez\",\n",
    "    \"lien en bio\", \"regardez\", \"suivez-moi\", \"partagez\", \"inscrivez-vous\",\n",
    "    \"expédition gratuite\", \"vente\", \"soldes\", \"offre limitée\", \"nouvelle offre\",\n",
    "    \"partenariat\", \"collaboration\", \"sponsorisé\", \"gagnez maintenant\",\n",
    "    \"cliquez ici\", \"achetez maintenant\", \"boutique officielle\", \"tirage au sort\",\n",
    "    \"mise en ligne\", \"lancement\", \"live maintenant\"\n",
    "    ]\n",
    "\n",
    "    # Binary: tweet contains ANY promo word\n",
    "    df[\"promo_words\"] = df[\"text\"].str.lower().apply(\n",
    "        lambda t: any(w in t for w in promo_words)\n",
    "    ).astype(int)\n",
    "\n",
    "    # Count: how many promo words appear in the tweet\n",
    "    df[\"num_promo_words\"] = df[\"text\"].str.lower().apply(\n",
    "        lambda t: sum(w in t for w in promo_words)\n",
    "    ).astype(int)\n",
    "\n",
    "    df[\"statuses\"] = df[\"user\"].apply(\n",
    "        lambda u: u.get(\"statuses_count\", -1) if isinstance(u, dict) else -1\n",
    "    )\n",
    "\n",
    "    df[\"account_age_days\"] = (pd.to_datetime(df[\"created_at\"]) -\n",
    "                          pd.to_datetime(df[\"user_created_at\"], errors=\"coerce\")).dt.days.fillna(-1)\n",
    "    df[\"tweet_frequency\"] = df[\"statuses\"] / (df[\"account_age_days\"] + 1)\n",
    "    \n",
    "\n",
    "    print(\"Loaded:\", path)\n",
    "    print(\"Rows:\", len(df))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5153432f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: data/Kaggle2025/train.jsonl\n",
      "Rows: 154914\n",
      "Train size: 139422\n",
      "Val size: 15492\n"
     ]
    }
   ],
   "source": [
    "train_path = \"data/Kaggle2025/train.jsonl\"\n",
    "\n",
    "\n",
    "df = load_jsonl(train_path)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.1,\n",
    "    random_state=42,\n",
    "    stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_df))\n",
    "print(\"Val size:\", len(val_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d9b2265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "Device count: 1\n",
      "Current device: 0\n",
      "GPU name: NVIDIA RTX 4000 Ada Generation\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertModel(\n",
       "  (embeddings): CamembertEmbeddings(\n",
       "    (word_embeddings): Embedding(32005, 768, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(1, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): CamembertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x CamembertLayer(\n",
       "        (attention): CamembertAttention(\n",
       "          (self): CamembertSdpaSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): CamembertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): CamembertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): CamembertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): CamembertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Present working\n",
    "print(\"Using CUDA:\", torch.cuda.is_available())\n",
    "print(\"Device count:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device() if torch.cuda.is_available() else \"CPU\")\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"CPU\")\n",
    "\n",
    "MODEL_NAME = \"camembert-base\"\n",
    "\n",
    "tokenizer = CamembertTokenizer.from_pretrained(MODEL_NAME)\n",
    "camembert = CamembertModel.from_pretrained(MODEL_NAME,\n",
    "                                           output_hidden_states = True)\n",
    "camembert.to(\"cuda\")\n",
    "camembert.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3498c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139422, 768)\n",
      "(15492, 768)\n"
     ]
    }
   ],
   "source": [
    "class EmbeddingDataset(Dataset):\n",
    "    def __init__(self, df, text_col=\"text\"):\n",
    "        self.texts = df[text_col].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        enc = tokenizer(\n",
    "            self.texts[idx],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=160,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {k: v.squeeze(0) for k, v in enc.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mean_pooling(hidden_state, attention_mask):\n",
    "    # hidden_state: (B, L, H)\n",
    "    # attention_mask: (B, L)\n",
    "    mask = attention_mask.unsqueeze(-1)       # (B, L, 1)\n",
    "    masked_hidden = hidden_state * mask       # zero-out PAD tokens\n",
    "\n",
    "    summed = masked_hidden.sum(dim=1)         # sum over tokens\n",
    "    counts = mask.sum(dim=1).clamp(min=1)     # avoid division by zero\n",
    "\n",
    "    return summed / counts                    # (B, H)\n",
    "\n",
    "\n",
    "def last_four_layers_pooling(hidden_states, attention_mask):\n",
    "\n",
    "    layers = hidden_states[-4:]  # list of 4 tensors\n",
    "\n",
    "    pooled = []\n",
    "    for hs in layers:\n",
    "        pooled.append(mean_pooling(hs, attention_mask))  # each is (B, H)\n",
    "\n",
    "    stacked = torch.stack(pooled, dim=0)  # (4, B, H)\n",
    "\n",
    "    # weights: more weight for deeper layers\n",
    "    weights = torch.tensor([1.0, 2.0, 3.0, 4.0], device=stacked.device)\n",
    "    weights = weights / weights.sum()                # normalize\n",
    "    weights = weights.view(4, 1, 1)                  # (4, 1, 1)\n",
    "\n",
    "    return (stacked * weights).sum(dim=0)\n",
    "\n",
    "    \n",
    "\n",
    "def extract_embeddings(df, batch_size=32):\n",
    "    ds = EmbeddingDataset(df)\n",
    "    dl = DataLoader(ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    for batch in dl:\n",
    "        input_ids = batch[\"input_ids\"].to(\"cuda\")\n",
    "        attn_mask = batch[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = camembert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attn_mask,\n",
    "                output_hidden_states=True,\n",
    "                return_dict=True\n",
    "            )\n",
    "\n",
    "            # use last 4 layers pooling\n",
    "            hidden_states = outputs.hidden_states\n",
    "            sentence_emb = last_four_layers_pooling(hidden_states, attn_mask)\n",
    "\n",
    "        all_embeddings.append(sentence_emb.cpu())\n",
    "\n",
    "    return torch.cat(all_embeddings, dim=0).numpy()\n",
    "\n",
    "\n",
    "\n",
    "emb_train = extract_embeddings(train_df)\n",
    "emb_val   = extract_embeddings(val_df)\n",
    "\n",
    "print(emb_train.shape)   # (N_train, 768)\n",
    "print(emb_val.shape)     # (N_val, 768)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1abbcb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"notebooks/emb_train.npy\", emb_train)\n",
    "# np.save(\"notebooks/emb_val.npy\", emb_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cb5ccb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train metadata shape: (139422, 214)\n",
      "Val metadata shape: (15492, 214)\n"
     ]
    }
   ],
   "source": [
    "def flatten_record(prefix, obj, out_dict):\n",
    "    if isinstance(obj, dict):\n",
    "        for k, v in obj.items():\n",
    "            new_key = f\"{prefix}_{k}\" if prefix else k\n",
    "            flatten_record(new_key, v, out_dict)\n",
    "    elif isinstance(obj, list):\n",
    "        out_dict[prefix + \"_len\"] = len(obj)\n",
    "    else:\n",
    "        out_dict[prefix] = obj\n",
    "\n",
    "\n",
    "def extract_features_from_row(row):\n",
    "    feature_dict = {}\n",
    "    for col in row.index:\n",
    "        if col in [\"text\", \"label\"]:  # skip text + label\n",
    "            continue\n",
    "        flatten_record(col, row[col], feature_dict)\n",
    "    return feature_dict\n",
    "\n",
    "\n",
    "def build_metadata_dataframe(df):\n",
    "    records = []\n",
    "    for _, row in df.iterrows():\n",
    "        records.append(extract_features_from_row(row))\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "\n",
    "meta_train = build_metadata_dataframe(train_df)\n",
    "meta_val   = build_metadata_dataframe(val_df)\n",
    "\n",
    "\n",
    "meta_val = meta_val.reindex(columns=meta_train.columns, fill_value=-1)\n",
    "\n",
    "\n",
    "print(\"Train metadata shape:\", meta_train.shape)\n",
    "print(\"Val metadata shape:\", meta_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "738b0474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final shapes: (139422, 79) (15492, 79)\n"
     ]
    }
   ],
   "source": [
    "# Remove constants\n",
    "constant_cols = meta_train.columns[meta_train.nunique() <= 1].tolist()\n",
    "meta_train.drop(columns=constant_cols, inplace=True)\n",
    "meta_val.drop(columns=constant_cols, inplace=True)\n",
    "\n",
    "# Remove very sparse\n",
    "sparse_cols = meta_train.columns[meta_train.isna().mean() > 0.80].tolist()\n",
    "meta_train.drop(columns=sparse_cols, inplace=True)\n",
    "meta_val.drop(columns=sparse_cols, inplace=True)\n",
    "\n",
    "# Remove datetime\n",
    "datetime_cols = meta_train.select_dtypes(include=[\"datetime64[ns]\", \"datetime64[ns, UTC]\"]).columns\n",
    "meta_train.drop(columns=datetime_cols, inplace=True)\n",
    "meta_val.drop(columns=datetime_cols, inplace=True)\n",
    "\n",
    "# Handle object columns\n",
    "object_cols = meta_train.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "keep = []\n",
    "drop = []\n",
    "\n",
    "for col in object_cols:\n",
    "    if meta_train[col].nunique() <= 20:\n",
    "        keep.append(col)\n",
    "    else:\n",
    "        drop.append(col)\n",
    "\n",
    "# Label encode small-categorical columns\n",
    "for col in keep:\n",
    "    le = LabelEncoder()\n",
    "    meta_train[col] = le.fit_transform(meta_train[col].astype(str))\n",
    "    meta_val[col]   = le.transform(meta_val[col].astype(str))\n",
    "\n",
    "# Drop high-cardinality object columns\n",
    "meta_train.drop(columns=drop, inplace=True)\n",
    "meta_val.drop(columns=drop, inplace=True)\n",
    "\n",
    "# Convert to numeric + fill NA\n",
    "meta_train = meta_train.apply(pd.to_numeric, errors=\"ignore\").fillna(-1)\n",
    "meta_val   = meta_val.apply(pd.to_numeric, errors=\"ignore\").fillna(-1)\n",
    "\n",
    "print(\"Final shapes:\", meta_train.shape, meta_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b24aaad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(139422, 847) (15492, 847)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.hstack([emb_train, meta_train.values])\n",
    "X_val   = np.hstack([emb_val, meta_val.values])\n",
    "\n",
    "y_train = train_df[\"label\"].values\n",
    "y_val   = val_df[\"label\"].values\n",
    "\n",
    "print(X_train.shape, X_val.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b550d06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB {'n_estimators': 600, 'max_depth': 6, 'learning_rate': 0.03} → acc=0.8430\n",
      "XGB {'n_estimators': 600, 'max_depth': 8, 'learning_rate': 0.02} → acc=0.8449\n",
      "XGB {'n_estimators': 900, 'max_depth': 6, 'learning_rate': 0.02} → acc=0.8442\n",
      "XGB {'n_estimators': 900, 'max_depth': 6, 'learning_rate': 0.03} → acc=0.8426\n",
      "XGB {'n_estimators': 900, 'max_depth': 8, 'learning_rate': 0.05} → acc=0.8479\n",
      "XGB {'n_estimators': 1200, 'max_depth': 8, 'learning_rate': 0.03} → acc=0.8470\n",
      "XGB {'n_estimators': 1200, 'max_depth': 6, 'learning_rate': 0.02} → acc=0.8435\n",
      "XGB {'n_estimators': 1200, 'max_depth': 8, 'learning_rate': 0.02} → acc=0.8468\n",
      "Best XGBoost accuracy: 0.8478569584301575\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_best = None\n",
    "xgb_best_acc = 0\n",
    "\n",
    "xgb_grid = [\n",
    "\n",
    "    {\"n_estimators\": 600,  \"max_depth\": 6, \"learning_rate\": 0.03},\n",
    "    {\"n_estimators\": 600, \"max_depth\": 8, \"learning_rate\": 0.02},\n",
    "    {\"n_estimators\": 900, \"max_depth\": 6, \"learning_rate\": 0.02},\n",
    "    {\"n_estimators\": 900, \"max_depth\": 6, \"learning_rate\": 0.03},\n",
    "    {\"n_estimators\": 900, \"max_depth\": 8, \"learning_rate\": 0.05},\n",
    "    {\"n_estimators\": 1200, \"max_depth\": 8, \"learning_rate\": 0.03},\n",
    "    {\"n_estimators\": 1200, \"max_depth\": 6, \"learning_rate\": 0.02},\n",
    "    {\"n_estimators\": 1200, \"max_depth\": 8, \"learning_rate\": 0.05},\n",
    "\n",
    "]\n",
    "\n",
    "\n",
    "for params in xgb_grid:\n",
    "    model = XGBClassifier(\n",
    "        n_estimators=params[\"n_estimators\"],\n",
    "        max_depth=params[\"max_depth\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.9,\n",
    "        eval_metric=\"logloss\",\n",
    "        \n",
    "        tree_method=\"gpu_hist\",\n",
    "        predictor=\"gpu_predictor\",\n",
    "        gpu_id=0)\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict_proba(X_val)[:, 1]\n",
    "    acc = ((preds >= 0.5).astype(int) == y_val).mean()\n",
    "\n",
    "    print(f\"XGB {params} → acc={acc:.4f}\")\n",
    "\n",
    "    if acc > xgb_best_acc:\n",
    "        xgb_best_acc = acc\n",
    "        xgb_best = model\n",
    "\n",
    "preds_xgb = xgb_best.predict_proba(X_val)[:, 1]\n",
    "print(\"Best XGBoost accuracy:\", xgb_best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28529203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 65016, number of negative: 74406\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 203103\n",
      "[LightGBM] [Info] Number of data points in the train set: 139422, number of used features: 847\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA RTX 4000 Ada Generation, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 818 dense feature groups (109.03 MB) transferred to GPU in 0.013977 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466325 -> initscore=-0.134903\n",
      "[LightGBM] [Info] Start training from score -0.134903\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "LGBM {'num_leaves': 64, 'learning_rate': 0.05, 'feature_fraction': 0.8} → acc=0.8444\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 65016, number of negative: 74406\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 203103\n",
      "[LightGBM] [Info] Number of data points in the train set: 139422, number of used features: 847\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA RTX 4000 Ada Generation, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 818 dense feature groups (109.03 MB) transferred to GPU in 0.017222 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466325 -> initscore=-0.134903\n",
      "[LightGBM] [Info] Start training from score -0.134903\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "LGBM {'num_leaves': 128, 'learning_rate': 0.03, 'feature_fraction': 0.9} → acc=0.8462\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 65016, number of negative: 74406\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 203103\n",
      "[LightGBM] [Info] Number of data points in the train set: 139422, number of used features: 847\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA RTX 4000 Ada Generation, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 818 dense feature groups (109.03 MB) transferred to GPU in 0.018261 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466325 -> initscore=-0.134903\n",
      "[LightGBM] [Info] Start training from score -0.134903\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "LGBM {'num_leaves': 128, 'learning_rate': 0.02, 'feature_fraction': 0.8} → acc=0.8470\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 65016, number of negative: 74406\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 203103\n",
      "[LightGBM] [Info] Number of data points in the train set: 139422, number of used features: 847\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA RTX 4000 Ada Generation, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 818 dense feature groups (109.03 MB) transferred to GPU in 0.013166 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466325 -> initscore=-0.134903\n",
      "[LightGBM] [Info] Start training from score -0.134903\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "LGBM {'num_leaves': 256, 'learning_rate': 0.02, 'feature_fraction': 0.9} → acc=0.8488\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 65016, number of negative: 74406\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 203103\n",
      "[LightGBM] [Info] Number of data points in the train set: 139422, number of used features: 847\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA RTX 4000 Ada Generation, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 818 dense feature groups (109.03 MB) transferred to GPU in 0.012423 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466325 -> initscore=-0.134903\n",
      "[LightGBM] [Info] Start training from score -0.134903\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "LGBM {'num_leaves': 256, 'learning_rate': 0.03, 'feature_fraction': 0.8} → acc=0.8494\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 65016, number of negative: 74406\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 203103\n",
      "[LightGBM] [Info] Number of data points in the train set: 139422, number of used features: 847\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA RTX 4000 Ada Generation, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 818 dense feature groups (109.03 MB) transferred to GPU in 0.014026 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466325 -> initscore=-0.134903\n",
      "[LightGBM] [Info] Start training from score -0.134903\n",
      "[LightGBM] [Warning] feature_fraction is set=1.0, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=1.0\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "LGBM {'num_leaves': 256, 'learning_rate': 0.03, 'feature_fraction': 1.0} → acc=0.8481\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 65016, number of negative: 74406\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 203103\n",
      "[LightGBM] [Info] Number of data points in the train set: 139422, number of used features: 847\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA RTX 4000 Ada Generation, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 818 dense feature groups (109.03 MB) transferred to GPU in 0.015418 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466325 -> initscore=-0.134903\n",
      "[LightGBM] [Info] Start training from score -0.134903\n",
      "[LightGBM] [Warning] feature_fraction is set=0.9, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "LGBM {'num_leaves': 512, 'learning_rate': 0.02, 'feature_fraction': 0.9} → acc=0.8504\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 65016, number of negative: 74406\n",
      "[LightGBM] [Info] This is the GPU trainer!!\n",
      "[LightGBM] [Info] Total Bins 203103\n",
      "[LightGBM] [Info] Number of data points in the train set: 139422, number of used features: 847\n",
      "[LightGBM] [Info] Using requested OpenCL platform 0 device 0\n",
      "[LightGBM] [Info] Using GPU Device: NVIDIA RTX 4000 Ada Generation, Vendor: NVIDIA Corporation\n",
      "[LightGBM] [Info] Compiling OpenCL Kernel with 256 bins...\n",
      "[LightGBM] [Info] GPU programs have been built\n",
      "[LightGBM] [Info] Size of histogram bin entry: 8\n",
      "[LightGBM] [Info] 818 dense feature groups (109.03 MB) transferred to GPU in 0.026023 secs. 1 sparse feature groups\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466325 -> initscore=-0.134903\n",
      "[LightGBM] [Info] Start training from score -0.134903\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "LGBM {'num_leaves': 512, 'learning_rate': 0.015, 'feature_fraction': 0.8} → acc=0.8506\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "Best LightGBM accuracy: 0.8506325845597728\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb_best = None \n",
    "lgb_best_acc = 0\n",
    "\n",
    "lgb_grid = [\n",
    "    {\"num_leaves\": 64, \"learning_rate\": 0.05, \"feature_fraction\": 0.8},\n",
    "    {\"num_leaves\": 128, \"learning_rate\": 0.03, \"feature_fraction\": 0.9},\n",
    "    {\"num_leaves\": 128, \"learning_rate\": 0.02, \"feature_fraction\": 0.8},\n",
    "    {\"num_leaves\": 256, \"learning_rate\": 0.02, \"feature_fraction\": 0.9},\n",
    "    {\"num_leaves\": 256, \"learning_rate\": 0.03, \"feature_fraction\": 0.8},\n",
    "    {\"num_leaves\": 256, \"learning_rate\": 0.03, \"feature_fraction\": 1.0},\n",
    "    {\"num_leaves\": 512, \"learning_rate\": 0.02, \"feature_fraction\": 0.9},\n",
    "    {\"num_leaves\": 512, \"learning_rate\": 0.015, \"feature_fraction\": 0.8},\n",
    "\n",
    "]\n",
    "\n",
    "for params in lgb_grid:\n",
    "    model = LGBMClassifier(\n",
    "    device = 'gpu',gpu_platform_id=0,\n",
    "    gpu_device_id=0,\n",
    "        num_leaves=params[\"num_leaves\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        n_estimators=1000,\n",
    "        feature_fraction=params[\"feature_fraction\"],\n",
    "        bagging_fraction=0.9,\n",
    "        bagging_freq=3,\n",
    "        objective=\"binary\"\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        eval_metric=\"logloss\",\n",
    "    )\n",
    "\n",
    "    preds = model.predict_proba(X_val)[:, 1]\n",
    "    acc = ((preds >= 0.5).astype(int) == y_val).mean()\n",
    "\n",
    "    print(f\"LGBM {params} → acc={acc:.4f}\")\n",
    "\n",
    "    if acc > lgb_best_acc:\n",
    "        lgb_best_acc = acc\n",
    "        lgb_best = model\n",
    "\n",
    "preds_lgb = lgb_best.predict_proba(X_val)[:, 1]\n",
    "print(\"Best LightGBM accuracy:\", lgb_best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d66e55a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CatBoost {'iterations': 1200, 'depth': 6, 'learning_rate': 0.02} → acc=0.8391\n",
      "CatBoost {'iterations': 1200, 'depth': 6, 'learning_rate': 0.05} → acc=0.8425\n",
      "CatBoost {'iterations': 1200, 'depth': 8, 'learning_rate': 0.02} → acc=0.8410\n",
      "CatBoost {'iterations': 1200, 'depth': 8, 'learning_rate': 0.05} → acc=0.8428\n",
      "CatBoost {'iterations': 1500, 'depth': 6, 'learning_rate': 0.02} → acc=0.8391\n",
      "CatBoost {'iterations': 1500, 'depth': 6, 'learning_rate': 0.05} → acc=0.8416\n",
      "CatBoost {'iterations': 1500, 'depth': 8, 'learning_rate': 0.02} → acc=0.8410\n",
      "CatBoost {'iterations': 1500, 'depth': 8, 'learning_rate': 0.05} → acc=0.8431\n",
      "Best CatBoost accuracy: 0.8430802995094242\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cat_best = None\n",
    "cat_best_acc = 0\n",
    "\n",
    "cat_grid = [\n",
    "    {\"iterations\": 1200, \"depth\": 6, \"learning_rate\": 0.02},\n",
    "    {\"iterations\": 1200, \"depth\": 6, \"learning_rate\": 0.05},\n",
    "    {\"iterations\": 1200, \"depth\": 8, \"learning_rate\": 0.02},\n",
    "    {\"iterations\": 1200, \"depth\": 8, \"learning_rate\": 0.05},\n",
    "    {\"iterations\": 1500, \"depth\": 6, \"learning_rate\": 0.02},\n",
    "    {\"iterations\": 1500, \"depth\": 6, \"learning_rate\": 0.05},\n",
    "    {\"iterations\": 1500, \"depth\": 8, \"learning_rate\": 0.02},\n",
    "    {\"iterations\": 1500, \"depth\": 8, \"learning_rate\": 0.05},\n",
    "]\n",
    "\n",
    "for params in cat_grid:\n",
    "    model = CatBoostClassifier(\n",
    "        iterations=params[\"iterations\"],\n",
    "        depth=params[\"depth\"],\n",
    "        learning_rate=params[\"learning_rate\"],\n",
    "        loss_function=\"Logloss\",\n",
    "        verbose=False,\n",
    "    )\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict_proba(X_val)[:, 1]\n",
    "    acc = ((preds >= 0.5).astype(int) == y_val).mean()\n",
    "\n",
    "    print(f\"CatBoost {params} → acc={acc:.4f}\")\n",
    "\n",
    "    if acc > cat_best_acc:\n",
    "        cat_best_acc = acc\n",
    "        cat_best = model\n",
    "\n",
    "preds_cat = cat_best.predict_proba(X_val)[:, 1]\n",
    "print(\"Best CatBoost accuracy:\", cat_best_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c112ccd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Ensemble Accuracy: 0.8490833978827782\n"
     ]
    }
   ],
   "source": [
    "p_final = (preds_xgb + preds_lgb + preds_cat) / 3\n",
    "acc_final = ((p_final >= 0.5).astype(int) == y_val).mean()\n",
    "\n",
    "print(\"Final Ensemble Accuracy:\", acc_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f25d701b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: data/Kaggle2025/kaggle_test.jsonl\n",
      "Rows: 103380\n"
     ]
    }
   ],
   "source": [
    "\n",
    "full_df = df.copy()   # df was the train.jsonl loaded at the start\n",
    "print(\"Full training size:\", len(full_df))\n",
    "\n",
    "emb_full = extract_embeddings(full_df)\n",
    "# np.save(\"emb_full.npy\", emb_full)\n",
    "\n",
    "test_df = load_jsonl(\"data/Kaggle2025/kaggle_test.jsonl\")\n",
    "\n",
    "emb_test = extract_embeddings(test_df)\n",
    "# np.save(\"emb_test.npy\", emb_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b23c5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_full: (154914, 847)\n",
      "X_test_final: (103380, 847)\n"
     ]
    }
   ],
   "source": [
    "# ==========================================================\n",
    "# 1) Build metadata for FULL TRAINING SET + TEST SET\n",
    "# ==========================================================\n",
    "meta_full = build_metadata_dataframe(full_df)\n",
    "meta_test = build_metadata_dataframe(test_df)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 2) Apply SAME cleaning rules learned from meta_train\n",
    "# ==========================================================\n",
    "\n",
    "# (A) Drop constant columns\n",
    "meta_full = meta_full.drop(columns=constant_cols, errors=\"ignore\")\n",
    "meta_test = meta_test.drop(columns=constant_cols, errors=\"ignore\")\n",
    "\n",
    "# (B) Drop sparse columns\n",
    "meta_full = meta_full.drop(columns=sparse_cols, errors=\"ignore\")\n",
    "meta_test = meta_test.drop(columns=sparse_cols, errors=\"ignore\")\n",
    "\n",
    "# (C) Drop datetime columns\n",
    "meta_full = meta_full.drop(columns=datetime_cols, errors=\"ignore\")\n",
    "meta_test = meta_test.drop(columns=datetime_cols, errors=\"ignore\")\n",
    "\n",
    "# (D) Drop same high-cardinality object columns\n",
    "meta_full = meta_full.drop(columns=drop, errors=\"ignore\")\n",
    "meta_test = meta_test.drop(columns=drop, errors=\"ignore\")\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 3) Label encode the SAME small-category columns (keep)\n",
    "# ==========================================================\n",
    "\n",
    "for col in keep:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    # Fit on TRAIN metadata only\n",
    "    le.fit(meta_train[col].astype(str))\n",
    "    \n",
    "    known_classes = set(le.classes_)\n",
    "    \n",
    "    # Replace unseen categories with a universal placeholder\n",
    "    meta_full[col] = meta_full[col].astype(str).apply(\n",
    "        lambda x: x if x in known_classes else '___UNKNOWN___'\n",
    "    )\n",
    "    meta_test[col] = meta_test[col].astype(str).apply(\n",
    "        lambda x: x if x in known_classes else '___UNKNOWN___'\n",
    "    )\n",
    "    \n",
    "    # Add placeholder to encoder classes\n",
    "    le.fit(list(known_classes) + ['___UNKNOWN___'])\n",
    "    \n",
    "    # Transform safely\n",
    "    meta_full[col] = le.transform(meta_full[col].astype(str))\n",
    "    meta_test[col] = le.transform(meta_test[col].astype(str))\n",
    "\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 4) Convert to numeric and fill missing\n",
    "# ==========================================================\n",
    "meta_full = meta_full.apply(pd.to_numeric, errors=\"ignore\").fillna(-1)\n",
    "meta_test = meta_test.apply(pd.to_numeric, errors=\"ignore\").fillna(-1)\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 5) Force FULL + TEST columns to match EXACTLY meta_train columns\n",
    "# ==========================================================\n",
    "\n",
    "# Add missing columns\n",
    "for col in meta_train.columns:\n",
    "    if col not in meta_full.columns:\n",
    "        meta_full[col] = 0\n",
    "    if col not in meta_test.columns:\n",
    "        meta_test[col] = 0\n",
    "\n",
    "# Drop extra columns\n",
    "extra_cols_full = [c for c in meta_full.columns if c not in meta_train.columns]\n",
    "meta_full = meta_full.drop(columns=extra_cols_full, errors=\"ignore\")\n",
    "\n",
    "extra_cols_test = [c for c in meta_test.columns if c not in meta_train.columns]\n",
    "meta_test = meta_test.drop(columns=extra_cols_test, errors=\"ignore\")\n",
    "\n",
    "# Reorder columns to match EXACT training order\n",
    "meta_full = meta_full[meta_train.columns]\n",
    "meta_test = meta_test[meta_train.columns]\n",
    "\n",
    "\n",
    "# ==========================================================\n",
    "# 6) Final matrices for full training + test prediction\n",
    "# ==========================================================\n",
    "X_full = np.hstack([emb_full, meta_full.values])\n",
    "y_full = full_df[\"label\"].values\n",
    "\n",
    "X_test_final = np.hstack([emb_test, meta_test.values])\n",
    "\n",
    "print(\"X_full:\", X_full.shape)\n",
    "print(\"X_test_final:\", X_test_final.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3129fb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_params = {\n",
    "    \"n_estimators\":  900 ,\n",
    "    \"max_depth\":     8 ,\n",
    "    \"learning_rate\": 0.05\n",
    "}\n",
    "\n",
    "best_lgb_params = {\n",
    "    \"num_leaves\":      512 ,\n",
    "    \"learning_rate\":   0.015 ,\n",
    "    \"feature_fraction\": 0.8\n",
    "}\n",
    "\n",
    "best_cat_params = {\n",
    "    \"iterations\":    1500 ,\n",
    "    \"depth\":         8 ,\n",
    "    \"learning_rate\": 0.05\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59161d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Training XGBoost\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_final = XGBClassifier(\n",
    "    n_estimators=best_xgb_params[\"n_estimators\"],\n",
    "    max_depth=best_xgb_params[\"max_depth\"],\n",
    "    learning_rate=best_xgb_params[\"learning_rate\"],\n",
    "    subsample=0.9,\n",
    "    colsample_bytree=0.9,\n",
    "    eval_metric=\"logloss\",\n",
    "    tree_method=\"hist\"\n",
    ")\n",
    "\n",
    "xgb_final.fit(X_full, y_full)\n",
    "pred_xgb_test = xgb_final.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c3df3e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n",
      "[LightGBM] [Info] Number of positive: 72240, number of negative: 82674\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.119199 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 203073\n",
      "[LightGBM] [Info] Number of data points in the train set: 154914, number of used features: 836\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.466323 -> initscore=-0.134911\n",
      "[LightGBM] [Info] Start training from score -0.134911\n",
      "[LightGBM] [Warning] feature_fraction is set=0.8, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.8\n",
      "[LightGBM] [Warning] bagging_fraction is set=0.9, subsample=1.0 will be ignored. Current value: bagging_fraction=0.9\n",
      "[LightGBM] [Warning] bagging_freq is set=3, subsample_freq=0 will be ignored. Current value: bagging_freq=3\n"
     ]
    }
   ],
   "source": [
    "# Final Training LGBM Classifier\n",
    "\n",
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgb_final = LGBMClassifier(\n",
    "    num_leaves=best_lgb_params[\"num_leaves\"],\n",
    "    learning_rate=best_lgb_params[\"learning_rate\"],\n",
    "    feature_fraction=best_lgb_params[\"feature_fraction\"],\n",
    "    n_estimators=1000,\n",
    "    bagging_fraction=0.9,\n",
    "    bagging_freq=3,\n",
    "    objective=\"binary\"\n",
    ")\n",
    "\n",
    "lgb_final.fit(X_full, y_full)\n",
    "pred_lgb_test = lgb_final.predict_proba(X_test_final)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "841325f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Training Catboost Classifier\n",
    "\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "cat_final = CatBoostClassifier(\n",
    "    iterations=best_cat_params[\"iterations\"],\n",
    "    depth=best_cat_params[\"depth\"],\n",
    "    learning_rate=best_cat_params[\"learning_rate\"],\n",
    "    loss_function=\"Logloss\",\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "cat_final.fit(X_full, y_full)\n",
    "pred_cat_test = cat_final.predict_proba(X_test_final)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d0449f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_probs = (pred_xgb_test + pred_lgb_test + pred_cat_test) / 3\n",
    "final_preds = (final_probs >= 0.5).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22915bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final_submission.csv\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame({\n",
    "    \"ID\": test_df[\"challenge_id\"],\n",
    "    \"Prediction\": final_preds\n",
    "})\n",
    "\n",
    "submission.to_csv(\"final_submission.csv\", index=False)\n",
    "print(\"Saved final_submission.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6e1d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "070c6265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CV_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
