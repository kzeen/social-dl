{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27609b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling libraries\n",
    "\n",
    "import re\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import stanza\n",
    "from pandas import json_normalize\n",
    "\n",
    "# Natural Language Processing (NLP) libraries\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Scikit-learn modeling libraries\n",
    "\n",
    "from sklearn.svm import LinearSVC, SDGClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88a78989",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = '../data/Kaggle2025/train.jsonl'\n",
    "df = pd.read_json(path, lines=True)\n",
    "df = json_normalize(df.to_dict(orient='records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5ca8783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install stanza\n",
    "\n",
    "# stanza.download(\"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b68e2f5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 23:29:55 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json: 435kB [00:00, 6.35MB/s]                    \n",
      "2025-11-17 23:29:55 INFO: Downloaded file to C:\\Users\\Asus\\stanza_resources\\resources.json\n",
      "2025-11-17 23:29:56 INFO: Loading these models for language: fr (French):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2025-11-17 23:29:56 INFO: Using device: cpu\n",
      "2025-11-17 23:29:56 INFO: Loading: tokenize\n",
      "2025-11-17 23:29:56 INFO: Loading: mwt\n",
      "2025-11-17 23:29:56 INFO: Loading: pos\n",
      "2025-11-17 23:29:58 INFO: Loading: lemma\n",
      "2025-11-17 23:29:58 INFO: Done loading processors!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    direct jean castex et olivier v√©ran annoncer d...\n",
       "1    direct jean castex et olivier v√©ran annoncer d...\n",
       "2    on √™tre de accord pour le cons√©quence √©conomiq...\n",
       "3    renforcer le capacit√© de d√©pistage et le actio...\n",
       "4    on moi dire dans le oreillette que le patient ...\n",
       "Name: lemmatized_text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(lang='fr', processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "\n",
    "def extract_text(row):\n",
    "    if pd.notna(row.get(\"extended_tweet.full_text\")):\n",
    "        return row[\"extended_tweet.full_text\"]\n",
    "    elif pd.notna(row.get(\"quoted_status.extended_tweet.full_text\")):\n",
    "        return row[\"quoted_status.extended_tweet.full_text\"]\n",
    "    elif pd.notna(row.get(\"quoted_status.text\")):\n",
    "        return row[\"quoted_status.text\"]\n",
    "    else:\n",
    "        return row.get(\"text\", \"\")\n",
    "\n",
    "df[\"clean_text\"] = df.apply(extract_text, axis=1)\n",
    "\n",
    "\n",
    "def clean_french_tweet(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"http\\S+\", \" \", text)               \n",
    "    text = re.sub(r\"@\\w+\", \" \", text)                  \n",
    "    text = re.sub(r\"#(\\w+)\", r\" \\1 \", text)            \n",
    "    text = re.sub(r\"[^\\w\\s√Ä-√ø]\", \" \", text)            \n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()           \n",
    "    return text\n",
    "df[\"clean_text\"] = df['clean_text'].apply(clean_french_tweet)\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    doc = nlp(text)\n",
    "    lemmas = [word.lemma for sent in doc.sentences for word in sent.words if word.lemma is not None]\n",
    "    return \" \".join(lemmas)\n",
    "df['lemmatized_text'] = df['clean_text'].apply(lemmatize_text)\n",
    "\n",
    "# print(\"cleaned French Text Samples: \")\n",
    "# print(df[\"clean_text\"].head())\n",
    "\n",
    "df['lemmatized_text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7aa8b0db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154914, 50000)\n"
     ]
    }
   ],
   "source": [
    "french_stopwords = stopwords.words('french')\n",
    "Vectorize = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=3,\n",
    "    max_features=50000,\n",
    "    stop_words=french_stopwords\n",
    ")\n",
    "X = Vectorize.fit_transform(df['clean_text'])\n",
    "print(X.shape)\n",
    "y = df['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9f207ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "Train size: 123931 | Val size: 30983\n"
     ]
    }
   ],
   "source": [
    "# Train-test splitting\n",
    "\n",
    "print(len(y) == X.shape[0])\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify = y, random_state = 40\n",
    ")\n",
    "\n",
    "print(\"Train size:\", X_train.shape[0], \"| Val size:\", X_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6c60327",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n",
      "Best C: 10\n",
      "\n",
      "Validation Accuracy: 0.6107865603718168\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.62      0.63     16535\n",
      "           1       0.58      0.60      0.59     14448\n",
      "\n",
      "    accuracy                           0.61     30983\n",
      "   macro avg       0.61      0.61      0.61     30983\n",
      "weighted avg       0.61      0.61      0.61     30983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Model design\n",
    "svm = LinearSVC(class_weight=\"balanced\", max_iter=5000)\n",
    "\n",
    "# Grid of C values (inverse of regularization strength)\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Grid search\n",
    "grid = GridSearchCV(\n",
    "    svm,\n",
    "    param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit on train data\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = grid.predict(X_val)\n",
    "\n",
    "print(\"Best C:\", grid.best_params_[\"C\"])\n",
    "print(\"\\nValidation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f65ee58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "Best Params: {'alpha': 1e-05, 'loss': 'hinge'}\n",
      "\n",
      "Validation Accuracy: 0.606106574573153\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.60      0.62     16535\n",
      "           1       0.57      0.62      0.59     14448\n",
      "\n",
      "    accuracy                           0.61     30983\n",
      "   macro avg       0.61      0.61      0.61     30983\n",
      "weighted avg       0.61      0.61      0.61     30983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Define model\n",
    "model = SGDClassifier(class_weight='balanced', max_iter=1000)\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'loss': ['hinge', 'squared_hinge'],\n",
    "    'alpha': [1e-3, 1e-4, 1e-5]  # Note: SGDClassifier uses 'alpha' = 1/C\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring='accuracy',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit to training set\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Params:\", grid.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = grid.predict(X_val)\n",
    "print(\"\\nValidation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b2ea21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAST TEXT Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "60bccde3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.4.0-cp311-cp311-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.7.0 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim) (1.15.3)\n",
      "Requirement already satisfied: smart_open>=1.8.1 in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from gensim) (7.5.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\asus\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
      "Downloading gensim-4.4.0-cp311-cp311-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 0.0/24.4 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 2.9/24.4 MB 14.0 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 6.0/24.4 MB 14.8 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 9.4/24.4 MB 15.1 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 12.6/24.4 MB 15.2 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 16.5/24.4 MB 15.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 20.4/24.4 MB 16.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  24.1/24.4 MB 16.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 24.4/24.4 MB 15.2 MB/s eta 0:00:00\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.1.1 -> 25.3\n",
      "[notice] To update, run: C:\\Users\\Asus\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc331b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Downloading FastText French vectors...\n",
      "‚úÖ Downloaded. Unzipping...\n",
      "‚úÖ Unzipped.\n",
      "üîÅ Loading word vectors into memory...\n",
      "üîÑ Converting tweets to vectors...\n",
      "‚úÖ Vector shape: (154914, 300)\n",
      "\n",
      "‚úÖ Validation Accuracy: 0.610076493560985\n",
      "\n",
      "üßæ Classification Report:\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.62      0.63     16535\n",
      "           1       0.58      0.60      0.59     14448\n",
      "\n",
      "    accuracy                           0.61     30983\n",
      "   macro avg       0.61      0.61      0.61     30983\n",
      "weighted avg       0.61      0.61      0.61     30983\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim.models.fasttext import load_facebook_vectors\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.downloader as api\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 2: Download Pretrained FastText French Embeddings\n",
    "# -----------------------------------------------\n",
    "# Download from fasttext.cc (https://fasttext.cc/docs/en/crawl-vectors.html)\n",
    "# French: cc.fr.300.vec.gz\n",
    "\n",
    "import urllib.request\n",
    "import gzip\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "fasttext_url = \"https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\"\n",
    "local_path = \"cc.fr.300.vec.gz\"\n",
    "vec_file = \"cc.fr.300.vec\"\n",
    "\n",
    "if not os.path.exists(vec_file):\n",
    "    print(\"‚è≥ Downloading FastText French vectors...\")\n",
    "    urllib.request.urlretrieve(fasttext_url, local_path)\n",
    "    print(\"‚úÖ Downloaded. Unzipping...\")\n",
    "    with gzip.open(local_path, 'rb') as f_in:\n",
    "        with open(vec_file, 'wb') as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "    print(\"‚úÖ Unzipped.\")\n",
    "\n",
    "# Load as KeyedVectors\n",
    "from gensim.models import KeyedVectors\n",
    "print(\"üîÅ Loading word vectors into memory...\")\n",
    "ft_model = KeyedVectors.load_word2vec_format(vec_file)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 3: Convert Each Tweet to Mean Word Vector\n",
    "# -----------------------------------------------\n",
    "\n",
    "def text_to_vector(text, model, dim=300):\n",
    "    tokens = simple_preprocess(text, deacc=True)  # tokenize and remove punct\n",
    "    vectors = [model[word] for word in tokens if word in model]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(dim)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "# Apply to all tweets\n",
    "print(\"üîÑ Converting tweets to vectors...\")\n",
    "X = np.vstack(df[\"clean_text\"].apply(lambda x: text_to_vector(x, ft_model)))\n",
    "y = df[\"label\"].values\n",
    "\n",
    "print(\"‚úÖ Vector shape:\", X.shape)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 4: Train/Validation Split\n",
    "# -----------------------------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 5: Train SVM\n",
    "# -----------------------------------------------\n",
    "model = LinearSVC(class_weight=\"balanced\", max_iter=5000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# -----------------------------------------------\n",
    "# Step 6: Evaluate\n",
    "# -----------------------------------------------\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "print(\"\\n‚úÖ Validation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(\"\\nüßæ Classification Report:\\n\")\n",
    "print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9972b2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
